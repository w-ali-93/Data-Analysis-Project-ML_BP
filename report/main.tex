
\documentclass[journal]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{filecontents}
\usepackage{natbib}
\usepackage{bibentry}
\usepackage{booktabs,chemformula}
\usepackage{lipsum}
\usepackage{multirow, tabularx, caption, booktabs}
\usepackage{makecell}
\renewcommand\cellalign{rc}
\usepackage{siunitx}
\usepackage{listings}
\sisetup{table-format=1.1}
\usepackage{color}
%\usepackage{biblatex}

%\usepackage{filecontents}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}


\usepackage{listings}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
%\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
%\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Multiclass classification of songs' genre using one-vs-all and multinomial logistic regression}

% make the title area
\maketitle


\begin{abstract}
The data analysis project, in the scope of Machine Learning Basic Principles course, focuses on multiclass classification to identify songs' genre using custom subset of the Million Song Dataset [1], and the labels obtained from AllMusic.com [2]. During the project, 3 differnt methods were used to optimze and improve the accuracy of results namely, 'K Nearest Neighbours (KNN) with Low Dimensional Embedding', 'Multinomial Logistic Regression' and 'One vs All (OvA) Logistic Regression'. The very first method 'KNN with LDE' employs Principal Component Analysis [5] to extract useful features out of the high-dimensional dataset and then uses KNN algorithm to create target clusters. While Multinomial and OvA method use probablistic models to classify target classes where the main difference occurs in the modeling assumptions behind each of the two models.


\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway.

% Note that keywords are not normally used for peerreview papers.
%\begin{IEEEkeywords}
%IEEEtran, journal, \LaTeX, paper, template.
%\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
Music, by no doubt, has become an imperative part of people's life. People prefer listening to their favorite genre of songs for more personalized experience. Because of the availability of tons of songs, listeners crave for more personalized experience while at the same time, they prefer non-static music for mood augmentation. \par
The project, in particular focuses on finding a pattern to identify the genre of songs. The identification pattern can help to build an online recommendation platform using exisiting databse of users so that listeners can have more customized experience while in search for new songs. During the entire project, we address the question that given a list of songs, how well can we analyze the audio to find a pattern that helps us identiy songs that belong to similar genre of music; which is to say that we construct a predictor h(x) for each genre (Y) using the features (x) of the songs and map it to a probability h(x). Simpler this task may seem for a certain type of songs (e.g. heavy metal vs classical), the classification problem can become difficult for other types (e.g.rock vs blues).

%\subsection{Subsection Heading Here}
%\blindtext

\section{Data Analysis}
The dataset used for the project is a custom subset of the Million Song Dataset [1], and the labels were obtained from AllMusic.com [2]. For simplicity, each song has been assigned only one label that corresponds to the most representative genre. The data is split into two datasets: a training data set with 4363 songs, and a test set dataset with 6544 songs. Each song has 264 features, and there are 10 possible classes in total, which are:
\begin{enumerate}
	\item Pop Rock
	\item Electronic
	\item Rap
	\item Jazz
	\item Latin
	\item RnB
	\item International
	\item Country
	\item Reggae
	\item Blues
\end{enumerate}

The Histogram of target labels below provides insights about the training data at hand: \par

\begin{figure}[ht!]
\begin{center}
	\includegraphics[width=\columnwidth]{images/hist.eps}
	\caption{Histogram of Target Labels}
	\label{fig: Fig.1}
\end{center}
\end{figure}
It is evident from the histogram that the training data at hand is imbalanced with a very high representation of class-1 datapoints.

\begin{figure}[h!]
	Fig.2 and Fig. 3 represent that layout of a single datapoint (i.e., a song) and strucutral reprensentation of example vector. \par
	\begin{center}
		\includegraphics[width=\columnwidth]{images/1_ExampleVectorStructure.PNG}
		\caption{Layout of Example Vector}
		\label{fig: Fig.2}
	\end{center}
\end{figure}
The feature vector for each song thus consists of 12 coefficients representing MFFCs (using 4 statistics each), 12 pitch classes representing Chroma (using 4 statistics each), and 24 bands representing Rythm Patterns (using 7 statistics each). The details on the structure of each component of the example vector is as below: \par

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=\columnwidth]{images/1_ExampleVectorStructure_2.PNG}
		\caption{Structural Representation of Example Vector}
		\label{fig:Fig.3}
	\end{center}
\end{figure}

%Since the feature data has high dimensionality, so it was decided to test an unsupervised learning approach to select create a lower dimensional dataset out of the existing dataset, to \textbf{potentially} improve performance and eliminate redundancy in the dataset. The approach adopted for this was \textbf{low dimensional embedding} [2]. \par
\section{Methods and experiments}
Multiclass classification problem was explored using three different methods: \par
\begin{itemize}
	\item KNN clustering with low dimensional embedding
	\item Multinomial logistic regression
	\item One-vs-All (OvA)
\end{itemize}

\subsection{KNN clustering with low dimensional embedding}
Low dimensional embedding with KNN clustering approach was adopted to check the possibility of reducing the high dimensions for performance and accuracy improvement.\par

The technique employs Principal Component Analysis to extract useful features out of the high-dimensional dataset, essentially projecting the features to a lower dimensional space, by retaining the top $n$ principal components which yield the highest explained variance. Next, it applies KNN clustering on the low-dimensional dataset, with the prior PCA helping to prevent the ML algorithm from succumbing to the curse of dimensionality [7]. \par

In the adopted approach, the dimensions of both our training and test datasets were redyced using PCA, followed by the application of the KNN algorithm to acquire a model that fits the samples to appropriate labels. The model was then applied to the training and test sets, and the accuracy for both applications was measured. \par

The approach was repeated for different values for dimensionality reduction and $K$ nearest neighbors, with the maximum accuracy finally being found at $K = 22$ and $n = 21$. The final accuracy results were less than satisfactory, although it was evident that the classification task was greatly expedited because of the reduced complexity of the lower dimensional dataset. \par

\textit{Due to the dissatisfactory results, it was decided to not pursue a dimensional reduction approach any further and rather switched to alternate methods for solving the problem.} \par

Before, we jump into the details of Multinomial and OvR Logistic Regresss, first let's try to understand the difference between both. The main difference occurs in the modeling assumptions behind each of the two models. Let's assume that there are K classes. For OVA, the assumption is that there are K independent classification problems, one for each class, i.e., for class i, we learn a logistic (probability) model of the form $\dfrac{1}{1+exp(-\beta_{i}^Tx)}$ and each of these K problems are independent of the other K-1 logistic regression problems. Whereas for Multinomial model, we assume that there is a multinomial conditional distribution,  such that for a class i we have the following logistic model $\dfrac{exp(-\beta_{i}^Tx)}{\sum_{j=1}^{K}exp(-\beta_{j}^Tx)}$. As can be seen immediately, learning $\beta_i$ for a particular class i immediately affects the model for other classes j through the joint model.\\

\subsection{Multinomial logistic regression}
Multinomial Logistic Regression [3] is the linear regression analysis technique used when the dependent variable is nominal with more than two levels. Thus it is an extension of logistic regression, which analyzes dichotomous (binary) dependents. One of the major reasons for choosing multinomial logistic regression is because it works well on big data irrespective of different areas. \par
For the data analysis project, same technique was used like the logistic regression for binary classification until calculating the probabilities for each target. Once the probabilities were calculated, these were transferred into one hot encoding and cross entropy method was used for calculating the properly optimized weights. The idea is to construct a linear predictor function that constructs a score from a set of weights that are linearly combined with the features of a given observation using a dot product: \par
\begin{equation}
	score(X_i,k) = \beta_k . X_i,
\end{equation}
where $X_i$ (in our case) is the vector that contains the training data of songs, $\beta_k$ is a vector of weights (or regression coefficients) corresponding to k=10 outcomes of songs' genre, and $score(X_i, k)$ is the score associated with assigning each observation i to category k. \par
The following inforgraph explains the idea very well \par
\begin{center}
	\includegraphics[width=\columnwidth]{images/how_it_works.png}
\end{center}

Just like the sigmoid function in binary logistic regresssion, softmax function was used in multinomial logistic regression. The Softmax function is a probabilistic function which calculates the probabilities for the given score and returns the high probability value for the high scores and fewer probabilities for the remaining scores. Therefore, each observation i in the training dataset (X) was evaluated on k=10 predictors (h(x)) and then, softmax function was used to find the highest probabilitic class to which the observation i belongs. \par
The last step used in the multinomial logistic regression is the cross-entropy. It is a distance calculation function which takes the calculated probabilities from softmax function and then creates one-hot-encoding matrix to calculate the distance. For the right target class, the distance value is less, and the distance values are larger for the wrong target class. \par

The major challenge faced with this particular technique was to get the logitic loss to converge. Different number of iterations and different implementations of multinomial logistic regeression were used. $sklearn.linear-model.LogisticRegression$ [2] function from scikit-learn library was also experimented with different number of iterations (upto 500 which takes around 5 to 10 minutes), and different solvers like ‘newton-cg’, ‘lbfgs’, ‘sag’, ‘saga’. The best result was obtained using 'newton-cg' solver.

\subsection{One vs All (OvA)}
The third approach explored for the project was One-vs-All (also called One-vs-Rest) Classification. One classifier was trained for every class, taking the samples for that particular class as positive, while the remaining samples are treated as negative. \par
The base classifer must yield a real-valued confidence score for the decision, as opposed to merely a class label, since discrete class labels could become ambiguous if results from multiple classifiers yield different labels for the same sample. \par

An algorithm describing the procedure is as follows:
\begin{itemize}
	\item\textit{Inputs to the algorithm:}
	\begin{itemize}
		\item dataset of samples $X$ and labels $y$, where $y_{i} \in \{1, 2, ..., N\}$ is the label for the particular sample $X_{i}$
		\item $L$, a binary classification training algorithm 
	\end{itemize}
	\item\textit{Outputs of the algorithm:}
	\begin{itemize}
		\item a set of classifiers $c_{n}$ for $n \in \{1, 2, ..., N\}$
	\end{itemize}
	\item\textit{Mechanism:}
	\begin{itemize}
		\item For every $n \in \{1, 2, ..., N\}$
		\begin{itemize}
			\item Build a separate label vector $z$, with $z_{i} = 1$ if $y_{i} = n$ and $z_{i} = 0$ for all other $y_{i}$
			\item Apply training algorithm $L$ on samples $X$ and label vector $z$, to get $c_{n}$ 
		\end{itemize}
	\end{itemize}
\end{itemize}\par
Concretely deciding which class an unseen sample $x$ belongs to requires the application of all the obtained classifiers $c_{n}$ to it, and then choosing the label $n$ whose classifier yields the largest confidence score:
\begin{center}
	${\hat{y}} =\underset{n\in\{1, 2, ..., N\}}{argmax} c_{n}(x)$
\end{center} \par
Two potential issues with the OVR are approach are:
\begin{itemize}
	\item The confidence values can be different in terms of scale between each binary classifier
	\item Despite the training set having a balanced class distribution, the binary classifiers will see it as unbalanced since the number of negative labels will (most likely) be much greater than the number of positive labels.
\end{itemize}
In the implementation of the OVR approach, the multinomial logistic regression algorithm was modified to generate $N$ separate classifiers. The final label for each is decided based on the classifier that results in the best confidence score.

\section{Results}
Table-1 indicates the results for accuracy while table 2 indicates results for LogLoss. Among the many attempts that were made, results for the few are summarized below: \par
\begin{table}[!htb]
	\captionsetup{size=footnotesize}
	\caption{\textbf{Accuracy}} \label{tab:fr1}
	\setlength\tabcolsep{0pt} % let LaTeX compute intercolumn whitespace
	\footnotesize\centering
	%This table provides the frequencies.
	
	\smallskip 
	\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}rcccr}
		\toprule
		Index & Method & Training Accuracy & Validation Accuracy & Kaggle Result \\
		\midrule
		1   & KNN with LDE   & 57.04\%    & 53.03\%     & NA \\
		2   & Multinomial   & 74.53\%    & 63.14\%     & 62.13\% \\
		3   & Multinomial   & 75.8\%     & 63.90\%     & 62.483\% \\
		4   & OvR   & 75.9\%     & 63.5\%     & 62.88\% \\
		\midrule
		Best & OvR & 75.9\%     & 63.5\%     & 62.88\% \\
		\bottomrule
	\end{tabular*}
\end{table}

\begin{table}[!htb]
	\captionsetup{size=footnotesize}
	\caption{\textbf{LogLoss}} \label{tab:fr2}
	\setlength\tabcolsep{0pt} % let LaTeX compute intercolumn whitespace
	\footnotesize\centering
	%This table provides the frequencies.
	
	\smallskip 
	\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}rcccr}
		\toprule
		Index  & Method & Training\verb!_!Accuracy & Validation\verb!_!Accuracy & Kaggle\verb!_!Result \\
		\midrule
		1   & Multinomial   & 75.8\%    & 63.90\%   & 2.60627 \\
		2   & OvR   & 75.8\%     & 63.90\%     & 3.07390 \\
		\midrule
		Best   & Multinomial   & 75.8\%    & 63.90\%   & 2.60627 \\
		\bottomrule
	\end{tabular*}
\end{table}

\section{Conclusion}
The very first method employed, 'K Nearest Neighbours with Low Dimensional Embedding' helped to reduce the dataset to a lower dimensional space and decrease the total run time for data analysis task but as a consequence, majority of dimensions present inside the dataset containing useful information got lost with the reduction of dimensionality that resulted in poor training and validation accuracy. Second method 'Multinomial Logistic Regression' worked well on the provided dataset but did not score more than 62.5\% accuracy results. In order to improve results, different parameters were tuned like maximum number of iterations, and solver (e.g., newton-cg, saga,sag) but results only improved to fractional poitns. The major reason seems to be the imbalanced dataset with higher number of representation of class-1 datapoints. Finally, OvA classification method with logistic regression was implemented, yielding comparable results to the Multinomial Logistic Regression. OvA yielded marginally better results for the accuracy metric, whereas MLR yielded better results for the log-loss metric. One particluarly interesting approach, for future purposes, could be to reduce the representation of class-1 datapoints to create a more balanced dataset among all classes and then use Multinomial and OvA logistic regression for target classification, with Low Dimensional Embedding as a pre-processing step for eliminating potential redundancy.
\par


%\ifCLASSOPTIONcaptionsoff
\newpage
%\fi


%\begin{comment}
%\begin{thebibliography}{9}
%	\bibitem{Million_Songs}	
%	"Million Song Dataset | scaling MIR research".
%	\textit{"https://labrosa.ee.columbia.edu/millionsong/"},
%	\bibitem{sklearn}
%	%title  = "sklearn.linearmodel.LogisticRegression",
%	%url    = "http://scikit-learn.org/stable/modules/generated/sklearn.linearmodel.LogisticRegression.html",
%\end{thebibliography}
%\end{comment}

\begin{thebibliography}{1}
	
	\bibitem{Million_Songs},
	"Million Song Dataset | scaling MIR research",
	url =\textit{"https://labrosa.ee.columbia.edu/millionsong/"},
	
	\bibitem{sklearn},
	"sklearn.linearmodel.LogisticRegression",
	url = \textit{"http://scikit-learn.org/stable/modules/generated/sklearn.linearmodel.LogisticRegression.html"},

	\bibitem{Waytolearn}	
	"2 ways to implement Multinomial Logistic Regression in Python",
	url =\textit{"http://dataaspirant.com/2017/05/15/implement-multinomial-logistic-regression-python/"},
	
	\bibitem{AllMusicCom}
	"AllMusic",
	url = \textit{"https://www.allmusic.com/"},

	\bibitem{PCA}
	"Using PCA for Dimensionality Reduction",
	url = \textit{'https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/ '},
	
	\bibitem{LDE}
	"Structure preserving embedding",
	author = "Shaw, B.; Jebara, T."
	year = "2009"
	url = \textit{"http://www.cs.columbia.edu/~jebara/papers/spe-icml09.pdf"}
	
	\bibitem{CoD}
	"scikit-learn: Clustering and the curse of dimensionality",
	url = "http://www.markhneedham.com/blog/2016/08/27/scikit-learn-clustering-and-the-curse-of-dimensionality/".
	
\end{thebibliography}

\newpage

\section{Appendices}
Important functions related to the project and source code is given below. Please feel free to comment for any improvement.
\begin{lstlisting}[language=Python, caption= KNN Clustering with Low Dimensional Embedding]
def KNN_with_LDE():
""" Load Raw Data """
print("loading data ...")
# Load the Training Data
data_path_x = "train_data.csv";
songs_data = pd.read_csv("train_data.csv", header=None);
# Load the True Labels
data_path_y = "train_labels.csv";
songs_labels = pd.read_csv("train_labels.csv", header=None);
data_path_z = "test_data.csv";
val_x = pd.read_csv("test_data.csv", header=None);

""" Pre Process Data """
print("preprocessing data ...")
scaler = preprocessing.StandardScaler().fit(songs_data)
scaler
scaler.mean_
scaler.scale_
scaler.transform(songs_data)
scaler.transform(val_x)

""" Split training data into training data and test data for cross validation """
print("partitioning data ...")
train_x, test_x, train_y, test_y = train_test_split(songs_data, songs_labels, train_size=0.8, random_state=0);

""" Perform feature reduction"""
print("performing feature reduction ...")
train_x_pca = PCA(21)
train_x_pca.fit(train_x)
train_x_reduced = train_x_pca.transform(train_x)
test_x_reduced = train_x_pca.transform(test_x)
val_x_reduced = train_x_pca.transform(val_x)

""" Build the KNN model"""
print("building model...")
n_neighbors = 22
weights = 'uniform'
clf = neighbors.KNeighborsClassifier(n_neighbors, weights)
clf.fit(train_x_reduced, np.asarray(train_y).ravel())

""" Output accuracy results for Train and Test partitions"""
print("Train Accuracy :: ", metrics.accuracy_score(train_y, clf.predict(train_x_reduced)));
print("Test Accuracy ::  ", metrics.accuracy_score(test_y, clf.predict(test_x_reduced)));

""" Predict the Test (unlabeled) Data """
test_data_results = mul_lr.predict(val_x_reduced);
""" Export the Test Data File """
df = pd.DataFrame(test_data_results);
df.to_csv('test_data_results.csv', index=False, header=False);
\end{lstlisting}

\begin{lstlisting}[language=Python, caption= Logistic Gradient Descent]
""" logistic_gradient_descent returns w_{opt}"""
def logistic_gradient_descent(x_vec,y_vec,N):

w = np.zeros((N,1));
w_opt = np.zeros((N,1));
a = 1e-5; """ Learning Rate """
yT = np.transpose(y_vec);
xT = np.transpose(x_vec);
error_margin = 0.0001;
e = 1e20;
_iter = 0;
min_error = e;
#for _iter in range(1000):
while(abs(e) > error_margin):
if (_iter > 1000):
break;
_num = math.exp(-np.dot(yT,np.dot(x_vec,w)));
_den = 1 + math.exp(-np.dot(yT,np.dot(x_vec,w)));
grad_w = -(5/N)*(_num/_den)*np.dot(xT,y_vec);
w = w - a*grad_w;
""" Computer Error """
e = math.log(1 + math.exp(-np.dot(yT,
			np.dot(x_vec,w))));

if (min_error > e):
min_error = e;
w_opt = w;
print (e);
_iter = _iter + 1;
#print (w.shape);
return (w_opt);
\end{lstlisting}

\begin{lstlisting}[language=Python, caption = Multinomial LogisticRegression]
""" Performs Multinomial Logistic Regression """

def mult_nomial_logr():
""" Prepare the Data """
# Load the Training Data
data_path_x = "path/train_data.csv";
songs_data = pd.read_csv
			(data_path_x, header=None);
# Lad the True Labels.
data_path_y = "path/train_labels.csv";
songs_target = pd.read_csv
			(data_path_y, header=None);
data_path_z = "path/test_data.csv";
test_data = pd.read_csv
			(data_path_z, header=None);

""" Split Data into Training and Validation """
train_x, test_x, train_y, test_y = 
train_test_split(songs_data, songs_target, 
			train_size=0.7, random_state=0);

"""Train multinomial logistic regression model"""
mul_lr = linear_model.LogisticRegression(multi_class='multinomial', max_iter=500, solver='netwon-cg').fit(train_x, train_y);
print ("Multinomial Logistic regression Train Accuracy :: ", metrics.accuracy_score(train_y, mul_lr.predict(train_x)));
print ("Multinomial Logistic regression Test Accuracy ::  ", metrics.accuracy_score(test_y, mul_lr.predict(test_x)));

""" Predict the Test Data """
test_data_results = mul_lr.predict(test_data);
""" Export the Test Data File """
df = pd.DataFrame(test_data_results);
df.to_csv('test_data_results.csv', index=False, header=False);
\end{lstlisting}
\begin{lstlisting}[language=Python, caption = OvR LogisticRegression]
""" Performs OvR Logistic Regression """
def mult_nomial_logr():
""" Prepare the Data """
# Load the Training Data
data_path_x = "path/train_data.csv";
songs_data = pd.read_csv(data_path_x, header=None);
# Lad the True Labels.
data_path_y = "path/train_labels.csv";
songs_target = pd.read_csv(data_path_y, header=None);
data_path_z = "path/test_data.csv";
test_data = pd.read_csv(data_path_z, header=None);

""" Split Data into Training and Validation """
train_x, test_x, train_y, test_y = train_test_split(songs_data, songs_target, train_size=0.7, random_state=0);

"""Train multinomial logistic regression model"""
mul_lr = linear_model.LogisticRegression(multi_class='ovr', max_iter=500, solver='netwon-cg').fit(train_x, train_y);
print ("OvR Logistic regression Train Accuracy :: ", metrics.accuracy_score(train_y, mul_lr.predict(train_x)));
print ("OvR Logistic regression Test Accuracy ::  ", metrics.accuracy_score(test_y, mul_lr.predict(test_x)));

""" Predict the Test Data """
test_data_results = mul_lr.predict(test_data);
""" Export the Test Data File """
df = pd.DataFrame(test_data_results);
df.to_csv('test_data_results.csv', index=False, header=False);
\end{lstlisting}




\end{document}